{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
    "from llama_index.embeddings import GradientEmbedding\n",
    "from llama_index.llms import GradientBaseModelLLM\n",
    "from llama_index.vector_stores import CassandraVectorStore\n",
    "from llama_index.readers.base import BaseReader\n",
    "from llama_index.schema import Document\n",
    "import llama_index\n",
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "from textwrap3 import wrap\n",
    "from IPython.display import Markdown, display\n",
    "from traceloop.sdk import Traceloop\n",
    "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import Ollama\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index.llms import Ollama\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.indices.postprocessor import SimilarityPostprocessor\n",
    "from llama_index import Prompt\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.prompts import PromptTemplate\n",
    "from llama_index.query_engine import CustomQueryEngine\n",
    "from llama_index.retrievers import BaseRetriever\n",
    "from llama_index.response_synthesizers import (\n",
    "    get_response_synthesizer,\n",
    "    BaseSynthesizer,\n",
    ")\n",
    "\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "import chromadb\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s).\n"
     ]
    }
   ],
   "source": [
    "documentsNassim = SimpleDirectoryReader(\"/mnt/nasmixprojects/books/nassimTalebDemo\").load_data()\n",
    "print(f\"Loaded {len(documentsNassim)} document(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama2\",base_url=\"http://192.168.1.232:11435\") #llm = Ollama(model=\"llama2\")\n",
    "#llm = Ollama(model=\"wizard-vicuna-uncensored\",base_url=\"http://192.168.1.232:11435\") #llm = Ollama(model=\"llama2\")\n",
    "#llm = Ollama(model=\"wizard-vicuna-uncensored\",base_url=\"http://192.168.1.148:11435\") #llm = Ollama(model=\"llama2\")\n",
    "\n",
    "#embed_model = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-base-en\")\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm = llm,\n",
    "    embed_model = embed_model,\n",
    "    chunk_size=256,\n",
    ")\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chroma only in RAM\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "chroma_collection = chroma_client.create_collection(\"quickstart\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chroma stored persisted in disk\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"nassim-demo\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_finance = VectorStoreIndex.from_documents( documentsNassim, storage_context=storage_context, service_context=service_context )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chromadb load from persisted disk\n",
    "db2 = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db2.get_or_create_collection(\"nassim-demo\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "index_finance = VectorStoreIndex.from_vector_store( vector_store, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_finance = VectorStoreIndex.from_documents( documentsNassim, storage_context=storage_context, service_context=service_context )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine3 = index_finance.as_query_engine(response_mode=\"tree_summarize\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what is the bed of procrustes?', 'what is the best way to keep the mind opened?', 'give a list of the 5 points to avoid narrow your mindset.']\n"
     ]
    }
   ],
   "source": [
    "#trulens\n",
    "eval_questions = []\n",
    "with open('generated_questions.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Remove newline character and convert to integer\n",
    "        item = line.strip()\n",
    "        eval_questions.append(item)\n",
    "\n",
    "print(eval_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "\n",
    "def run_evals(eval_questions, tru_recorder, query_engine):\n",
    "    for question in eval_questions:\n",
    "        with tru_recorder as recording:\n",
    "            response = query_engine.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = (\n",
    "    \"We have provided trusted context information below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given this trusted and cientific information, please answer the question: {query_str}. Remember that the statements of the context are verfied and come from trusted sources.\\n\"\n",
    ")\n",
    "qa_template = Prompt(template)\n",
    "\n",
    "new_summary_tmpl_str = (\n",
    "    \"The original query is as follows: {query_str}\"\n",
    "    \"We have provided an existing answer: {existing_answer}\"\n",
    "    \"We have the opportunity to refine the existing answer (only if needed) with some more trusted context below. Remember that the statements of the context are verfied and come from trusted sources.\"\n",
    "    \"------------\"\n",
    "    \"{context_msg}\"\n",
    "    \"------------\"\n",
    "    \"Given the new trusted context, refine the original answer to better answer the query. If the context isn't useful, return the original answer. Remember that the statements of the new context are verfied and come from trusted sources.\"\n",
    "    \"Refined Answer: sure thing! \"\n",
    ")\n",
    "new_summary_tmpl = PromptTemplate(new_summary_tmpl_str)\n",
    "\n",
    "#https://gpt-index.readthedocs.io/en/v0.6.27/how_to/customization/custom_prompts.html\n",
    "#query_engine2 = restored_index.as_query_engine(text_qa_template=qa_template, similarity_top_k=24)\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index_finance,\n",
    "    similarity_top_k=12,\n",
    ")\n",
    "#https://gpt-index.readthedocs.io/en/v0.7.11/core_modules/model_modules/prompts.html\n",
    "response_synthesizer = get_response_synthesizer( ##try compact?\n",
    "    text_qa_template=qa_template,\n",
    "    refine_template=new_summary_tmpl\n",
    ")\n",
    "query_engine3 = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    # node_postprocessors=[\n",
    "    #     SimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "    # ]\n",
    ")\n",
    "\n",
    "response = query_engine3.query(\"make a list of things to do to avoid over simplifying and being narrow minded?\")\n",
    "print (response)\n",
    "\n",
    "\n",
    "# for question in eval_questions:\n",
    "#     print(question)\n",
    "#     print(query_engine3.query(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trulens_eval.feedback.provider.litellm import LiteLLM\n",
    "\n",
    "#llm = Ollama(model=\"wizard-vicuna-uncensored\",base_url=\"http://192.168.1.232:11435\")\n",
    "litellm_provider = LiteLLM(model_engine=\"ollama/llama2\", endpoint=\"http://192.168.1.232:11435\")\n",
    "litellm_provider.relevance(\"what color is the white house?\",\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n",
      "âœ… In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In Context Relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Context Relevance, input statement will be set to __record__.app.retriever.retrieve.rets[:].node.text .\n",
      "âœ… In Groundedness, input source will be set to __record__.app.retriever.retrieve.rets[:].node.text.collect() .\n",
      "âœ… In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "what is the bed of procrustes?\n",
      "what is the best way to keep the mind opened?\n",
      "give a list of the 5 points to avoid narrow your mindset.\n",
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3488e06b9a450cae3be215122635f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valuâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from trulens_eval.feedback.provider.litellm import LiteLLM\n",
    "\n",
    "from litellm import completion\n",
    "\n",
    "from trulens_eval import (\n",
    "    Feedback,\n",
    "    TruLlama,\n",
    "    Select\n",
    ")\n",
    "\n",
    "from trulens_eval.feedback import Groundedness\n",
    "\n",
    "from trulens_eval import Tru\n",
    "\n",
    "\n",
    "from trulens_eval.utils.threading import TP\n",
    "TP.DEBUG_TIMEOUT = None # None to disable\n",
    "\n",
    "\n",
    "tru = Tru()\n",
    "tru.reset_database()\n",
    "\n",
    "# response = completion(\n",
    "#     model=\"ollama/wizard-vicuna-uncensored\", \n",
    "#     messages=[{ \"content\": \"respond in 20 words. who are you?\",\"role\": \"user\"}], \n",
    "#     api_base=\"http://192.168.1.232:11435\"\n",
    "# )\n",
    "# print(response)\n",
    "\n",
    "LiteLLM.set_verbose=True\n",
    "\n",
    "#itellm_provider = LiteLLM(model_engine=\"ollama/llama2\", endpoint=\"http://192.168.1.232:11435\")\n",
    "litellm_provider = LiteLLM(model_engine=\"ollama/llama2\", api_base='http://192.168.1.232:11435')\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "################################################################################################\n",
    "f_qa_relevance = Feedback(\n",
    "    litellm_provider.relevance_with_cot_reasons,\n",
    "    name=\"Answer Relevance\"\n",
    ").on_input_output()\n",
    "\n",
    "#context_selection = TruLlama.select_source_nodes().node.text\n",
    "context_selection = Select.RecordCalls.retriever.retrieve.rets[:].node.text\n",
    "\n",
    "f_qs_relevance = (\n",
    "    Feedback(litellm_provider.qs_relevance_with_cot_reasons,\n",
    "             name=\"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on(context_selection)\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=litellm_provider)\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons,\n",
    "             name=\"Groundedness\"\n",
    "            )\n",
    "    .on(context_selection.collect())\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "################################################################################################\n",
    "\n",
    "tru_recorder = TruLlama(\n",
    "    query_engine3,\n",
    "    app_id=\"App_1\",\n",
    "    feedbacks=[\n",
    "        f_qa_relevance,\n",
    "        f_qs_relevance,\n",
    "        f_groundedness\n",
    "    ],\n",
    "    feedback_mode = \"with_app\" #\"deffered\"\n",
    ")\n",
    "\n",
    "for question in eval_questions:\n",
    "    with tru_recorder as recording:\n",
    "        print(question)\n",
    "        query_engine3.query(question)\n",
    "\n",
    "################################################################################################\n",
    "records, feedback = tru.get_records_and_feedback(app_ids=[])\n",
    "records.head()\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "records[[\"input\", \"output\"] + feedback]\n",
    "\n",
    "tru.get_leaderboard(app_ids=[])\n",
    "\n",
    "tru.run_dashboard()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import get_prebuilt_trulens_recorder\n",
    "\n",
    "\n",
    "import importlib\n",
    "\n",
    "# # Assuming the 'utils.py' file is in the same directory as the code executing this import\n",
    "# utils_module = importlib.util.spec_from_file_location(\"utils\", \"utils.py\")\n",
    "# utils = importlib.util.module_from_spec(utils_module)\n",
    "# utils_module.loader.exec_module(utils)\n",
    "# get_prebuilt_trulens_recorder = utils.get_prebuilt_trulens_recorder\n",
    "\n",
    "\n",
    "from trulens_eval import Tru\n",
    "\n",
    "Tru().reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = (\n",
    "    \"We have provided trusted context information below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given this trusted and cientific information, please answer the question: {query_str}. Remember that the statements of the context are verfied and come from trusted sources.\\n\"\n",
    ")\n",
    "qa_template = Prompt(template)\n",
    "\n",
    "new_summary_tmpl_str = (\n",
    "    \"The original query is as follows: {query_str}\"\n",
    "    \"We have provided an existing answer: {existing_answer}\"\n",
    "    \"We have the opportunity to refine the existing answer (only if needed) with some more trusted context below. Remember that the statements of the context are verfied and come from trusted sources.\"\n",
    "    \"------------\"\n",
    "    \"{context_msg}\"\n",
    "    \"------------\"\n",
    "    \"Given the new trusted context, refine the original answer to better answer the query. If the context isn't useful, return the original answer. Remember that the statements of the new context are verfied and come from trusted sources.\"\n",
    "    \"Refined Answer: sure thing! \"\n",
    ")\n",
    "new_summary_tmpl = PromptTemplate(new_summary_tmpl_str)\n",
    "\n",
    "#https://gpt-index.readthedocs.io/en/v0.6.27/how_to/customization/custom_prompts.html\n",
    "#query_engine2 = restored_index.as_query_engine(text_qa_template=qa_template, similarity_top_k=24)\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index_finance,\n",
    "    similarity_top_k=12,\n",
    ")\n",
    "#https://gpt-index.readthedocs.io/en/v0.7.11/core_modules/model_modules/prompts.html\n",
    "response_synthesizer = get_response_synthesizer( ##try compact?\n",
    "    text_qa_template=qa_template,\n",
    "    refine_template=new_summary_tmpl\n",
    ")\n",
    "query_engine3 = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    # node_postprocessors=[\n",
    "    #     SimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "    # ]\n",
    ")\n",
    "\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"<br><br>\"))\n",
    "        \n",
    "prompts_dict = query_engine3.get_prompts()\n",
    "display_prompt_dict(prompts_dict)\n",
    "print(\"????????????????????????\")\n",
    "\n",
    "#response = query_engine3.query(\"what is the best advice to start investing?\")\n",
    "#response = query_engine3.query(\"how to be antifragile?\")\n",
    "#response = query_engine3.query(\"how to be antifragile? make a list with the 10 points more important\")\n",
    "#esponse = query_engine3.query(\"why is the yuan value so low in comparison with the dollar? make a list with the points that influence this fact\")\n",
    "#response = query_engine3.query(\"what is antifragility?\")\n",
    "#response = query_engine3.query(\"what is the bed of procrustes?\")\n",
    "response = query_engine3.query(\"make a list of things to do to avoid over simplifying and being narrow minded?\")\n",
    "\n",
    "display(Markdown(f\"<b>{response}</b>\"))\n",
    "\n",
    "vid_names = {}\n",
    "print(\"xxxxxxxxx\")\n",
    "nodes=response.source_nodes\n",
    "for node in nodes:\n",
    "    #print(node)\n",
    "    print(node.score)\n",
    "    #pprint.pprint(node.metadata)\n",
    "    vid_names.setdefault(node.metadata['file_name'], 0)\n",
    "    vid_names[node.metadata['file_name']] = vid_names[node.metadata['file_name']] + 1\n",
    "    print(node.metadata['file_name'])\n",
    "\n",
    "pprint.pprint(vid_names)\n",
    "print(\"--------\")\n",
    "print(response.get_formatted_sources(length=20000))\n",
    "print(\"!!!!!!!!!\")\n",
    "\n",
    "wrapped_text = wrap(response.get_formatted_sources(length=20000), width=80)\n",
    "for line in wrapped_text:\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v-translations-llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
